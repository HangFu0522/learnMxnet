{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归模型\n",
    "#### 模型描述\n",
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>y</mi>\n",
    "  <mo>=</mo>\n",
    "  <mi>X</mi>\n",
    "  <mo>&#x22C5;<!-- ⋅ --></mo>\n",
    "  <mi>w</mi>\n",
    "  <mo>+</mo>\n",
    "  <mi>b</mi>\n",
    "  <mo>+</mo>\n",
    "  <mi>&#x03B7;<!-- η --></mi>\n",
    "  <mo>,</mo>\n",
    "  <mspace width=\"1em\" />\n",
    "  <mtext>for&#xA0;</mtext>\n",
    "  <mi>&#x03B7;<!-- η --></mi>\n",
    "  <mo>&#x223C;<!-- ∼ --></mo>\n",
    "  <mrow class=\"MJX-TeXAtom-ORD\">\n",
    "    <mi class=\"MJX-tex-caligraphic\" mathvariant=\"script\">N</mi>\n",
    "  </mrow>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mn>0</mn>\n",
    "  <mo>,</mo>\n",
    "  <msup>\n",
    "    <mi>&#x03C3;<!-- σ --></mi>\n",
    "    <mn>2</mn>\n",
    "  </msup>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先生成训练的数据,这里先生成一个Y 然后利用Y的shape去生成随机噪声，防止输入错误导致广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd as ad\n",
    "\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "\n",
    "true_w = nd.array([2, -3.4])\n",
    "true_b = 4.2\n",
    "X=nd.random_normal(0,1,shape=(num_examples,num_inputs))\n",
    "Y=nd.dot(X,true_w)+true_b\n",
    "Y=Y+0.1*nd.random_normal(0,0.1,shape=Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "使用洗牌法，随机抽取训练的数据，一次的抽取量为bath_size，这里就代表以bath_size个样本做一次梯度下降训练。这里注意nd.take(array,index)用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "bath_size=10\n",
    "def data_iter():\n",
    "    idx=list(range(num_examples))\n",
    "    random.shuffle(idx)\n",
    "    for i in range(0,num_examples,bath_size):\n",
    "        j=nd.array(idx[i:min(i+bath_size,num_examples)])\n",
    "        yield nd.take(X,j),nd.take(Y,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机化初始值，并未每个参数生成导数空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w=nd.random_normal(0,1,shape=(2,1))\n",
    "b = nd.zeros((1,))\n",
    "\n",
    "params=[w,b]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型：输出=function(输入)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    return nd.dot(X, w) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义损失函数，使用最简单的平方根损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def square_loss(yhat,y):\n",
    "    return (yhat-y.reshape(yhat.shape))**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义优化函数，使用梯度下降算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SGD(params,lr):\n",
    "    for param in params:\n",
    "        param[:]=param-lr*param.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.65511134212\n",
      "0.00101087701536\n",
      "0.00101263489138\n",
      "0.00101048111435\n",
      "0.00100880371261\n"
     ]
    }
   ],
   "source": [
    "epoch=5\n",
    "learn_rate=0.01\n",
    "for e in range(epoch):\n",
    "    total_loss=0\n",
    "    num=0\n",
    "    for data_X,data_Y in data_iter():\n",
    "        with ad.record():\n",
    "            yhat=net(data_X)\n",
    "            loss=square_loss(yhat,data_Y)\n",
    "        loss.backward()\n",
    "        SGD(params,learn_rate)\n",
    "        total_loss += nd.sum(loss).asscalar() \n",
    "        num+=1\n",
    "    print(total_loss/num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params[0]: \n",
      "[[ 2.00021958]\n",
      " [-3.39915419]]\n",
      "<NDArray 2x1 @cpu(0)>\n",
      "true_w: \n",
      "[ 2.        -3.4000001]\n",
      "<NDArray 2 @cpu(0)>\n",
      "params[0]: \n",
      "[ 4.20022488]\n",
      "<NDArray 1 @cpu(0)>\n",
      "true_w: 4.2\n"
     ]
    }
   ],
   "source": [
    "print(\"params[0]:\",params[0])\n",
    "print(\"true_w:\",true_w)\n",
    "print(\"params[0]:\",params[1])\n",
    "print(\"true_w:\",true_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先生成一组数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd as ad\n",
    "from mxnet import gluon\n",
    "num_inputs = 2\n",
    "num_examples = 1000\n",
    "\n",
    "true_w = nd.array([2, -3.4])\n",
    "true_b = 4.2\n",
    "X=nd.random_normal(0,1,shape=(num_examples,num_inputs))\n",
    "Y=nd.dot(X,true_w)+true_b\n",
    "Y=Y+0.1*nd.random_normal(0,0.1,shape=Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = gluon.data.ArrayDataset(X, Y)：数据打包成一个训练集\n",
    "\n",
    "data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True):从训练集合中得到数据的生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "dataset = gluon.data.ArrayDataset(X, Y)\n",
    "data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用gluon.nn.Sequential()函数返回一个网络，这个时候网络没有任何操作，这里网络类似于层的容器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net=gluon.nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用gluon.nn.Dense(1)函数返回一个层，这里输入参数的个数可以由网络在接受输入的时候或者上一层的信息推断，因此我们只要给出输出参数个数即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用gluon.nn.Dense()生成一层后，我们使用网络的add()方法添加进net这个网络中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.add(gluon.nn.Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在构建好网络后（即向net这个容器中添加完毕层后），使用net的initialize()方法，这样可以初始化整个网络，这里包含了对导数的分配等等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_function=gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里使用gluon.Trainer(参数集，优化方法，其他参数)定义一个训练器。我们借助这个训练器去训练网络的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Trainer=gluon.Trainer(net.collect_params(),'sgd',{'learning_rate':0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其余都不变，这里需要注意的是训练的过程我们不在使用自己写的SGD函数手动训练,而是使用先前定义的训练器Trainer。Trainer.step(batch_size)会自动训练网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 0.023016\n",
      "Epoch 1, average loss: 0.002710\n",
      "Epoch 2, average loss: 0.000224\n",
      "Epoch 3, average loss: 0.000018\n",
      "Epoch 4, average loss: 0.000007\n"
     ]
    }
   ],
   "source": [
    "epochs=5\n",
    "batch_size=10\n",
    "for e in range(epochs):\n",
    "    total_loss=0\n",
    "    for data,label in data_iter:\n",
    "        with ad.record():\n",
    "            output=net(data)\n",
    "            loss=loss_function(output,label)\n",
    "        loss.backward()\n",
    "        Trainer.step(batch_size)\n",
    "        total_loss=nd.sum(loss).asscalar()\n",
    "    print(\"Epoch %d, average loss: %f\" % (e, total_loss/num_examples))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
