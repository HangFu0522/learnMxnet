{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "from mxnet import ndarray as nd\n",
    "\n",
    "def transform(data, label):\n",
    "    return data.astype('float32')/255, label.astype('float32')\n",
    "mnist_train = gluon.data.vision.FashionMNIST(train=True, transform=transform)\n",
    "mnist_test = gluon.data.vision.FashionMNIST(train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "train_data=gluon.data.DataLoader(mnist_train,batch_size=batch_size,shuffle=True)\n",
    "test_data=gluon.data.DataLoader(mnist_test,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keepdims=True 用于保证张量的维度特性，防止某一维度的长度为1时被自动降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp=nd.exp(x)\n",
    "    sum_=exp.sum(axis=1,keepdims=True)\n",
    "    return exp/sum_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(X):\n",
    "    return nd.maximum(X,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_input=28*28\n",
    "num_hide=256\n",
    "num_output=10\n",
    "weight_scale=0.1\n",
    "W1=nd.random_normal(shape=(num_input,num_hide),scale=weight_scale)\n",
    "B1=nd.random_normal(shape=(num_hide))\n",
    "W2=nd.random_normal(shape=(num_hide,num_output),scale=weight_scale)\n",
    "B2=nd.random_normal(shape=(num_output))\n",
    "params=[W1,B1,W2,B2]\n",
    "for param in params:\n",
    "    param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def net(X):\n",
    "    output1=nd.dot(X.reshape((-1,num_input)),W1)+B1\n",
    "    hide=relu(output1)\n",
    "    return softmax(nd.dot(hide,W2)+B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy(yhat,y):\n",
    "    return - nd.pick(nd.log(yhat),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(output, label):\n",
    "    return nd.mean(output.argmax(axis=1)==label).asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc=.0\n",
    "    for data,label in data_iterator:\n",
    "        output=net(data)\n",
    "        acc+=accuracy(output,label)\n",
    "    return acc/len(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1033203125"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_accuracy(test_data,net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import SGD\n",
    "from mxnet import autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 0.393821, Train acc 0.859081, Test acc 0.866309\n",
      "Epoch 1. Loss: 0.380344, Train acc 0.863725, Test acc 0.866992\n",
      "Epoch 2. Loss: 0.368827, Train acc 0.868794, Test acc 0.869727\n",
      "Epoch 3. Loss: 0.363345, Train acc 0.869171, Test acc 0.870605\n",
      "Epoch 4. Loss: 0.350662, Train acc 0.874756, Test acc 0.874023\n",
      "Epoch 5. Loss: 0.346111, Train acc 0.875975, Test acc 0.870508\n",
      "Epoch 6. Loss: 0.338737, Train acc 0.878613, Test acc 0.877637\n",
      "Epoch 7. Loss: 0.331218, Train acc 0.880652, Test acc 0.875195\n",
      "Epoch 8. Loss: 0.325888, Train acc 0.882264, Test acc 0.871289\n",
      "Epoch 9. Loss: 0.321658, Train acc 0.885073, Test acc 0.876855\n",
      "Epoch 10. Loss: 0.316386, Train acc 0.886370, Test acc 0.877637\n",
      "Epoch 11. Loss: 0.311219, Train acc 0.887627, Test acc 0.879297\n",
      "Epoch 12. Loss: 0.306644, Train acc 0.889672, Test acc 0.877832\n",
      "Epoch 13. Loss: 0.302566, Train acc 0.892110, Test acc 0.882520\n",
      "Epoch 14. Loss: 0.298764, Train acc 0.892681, Test acc 0.879004\n",
      "Epoch 15. Loss: 0.295256, Train acc 0.893988, Test acc 0.879297\n",
      "Epoch 16. Loss: 0.290781, Train acc 0.895180, Test acc 0.884668\n",
      "Epoch 17. Loss: 0.285936, Train acc 0.896482, Test acc 0.875879\n",
      "Epoch 18. Loss: 0.282981, Train acc 0.898731, Test acc 0.880859\n",
      "Epoch 19. Loss: 0.281140, Train acc 0.899557, Test acc 0.886230\n",
      "Epoch 20. Loss: 0.275825, Train acc 0.899961, Test acc 0.885254\n",
      "Epoch 21. Loss: 0.274158, Train acc 0.899900, Test acc 0.886621\n",
      "Epoch 22. Loss: 0.271927, Train acc 0.901989, Test acc 0.884961\n",
      "Epoch 23. Loss: 0.269194, Train acc 0.903119, Test acc 0.889160\n",
      "Epoch 24. Loss: 0.264199, Train acc 0.905380, Test acc 0.883105\n",
      "Epoch 25. Loss: 0.262146, Train acc 0.905413, Test acc 0.890137\n",
      "Epoch 26. Loss: 0.258605, Train acc 0.906871, Test acc 0.886621\n",
      "Epoch 27. Loss: 0.255942, Train acc 0.908173, Test acc 0.890332\n",
      "Epoch 28. Loss: 0.254323, Train acc 0.908649, Test acc 0.890430\n",
      "Epoch 29. Loss: 0.250964, Train acc 0.909663, Test acc 0.884766\n",
      "Epoch 30. Loss: 0.247521, Train acc 0.911209, Test acc 0.890625\n",
      "Epoch 31. Loss: 0.246038, Train acc 0.911082, Test acc 0.891992\n",
      "Epoch 32. Loss: 0.243205, Train acc 0.911447, Test acc 0.889453\n",
      "Epoch 33. Loss: 0.242658, Train acc 0.912699, Test acc 0.892090\n",
      "Epoch 34. Loss: 0.239634, Train acc 0.913204, Test acc 0.892090\n",
      "Epoch 35. Loss: 0.237906, Train acc 0.913276, Test acc 0.892480\n",
      "Epoch 36. Loss: 0.235906, Train acc 0.914600, Test acc 0.889551\n",
      "Epoch 37. Loss: 0.233857, Train acc 0.914993, Test acc 0.880762\n",
      "Epoch 38. Loss: 0.229117, Train acc 0.918096, Test acc 0.891406\n",
      "Epoch 39. Loss: 0.226091, Train acc 0.918102, Test acc 0.890918\n",
      "Epoch 40. Loss: 0.227143, Train acc 0.917370, Test acc 0.889941\n",
      "Epoch 41. Loss: 0.222957, Train acc 0.920002, Test acc 0.889941\n",
      "Epoch 42. Loss: 0.223823, Train acc 0.919731, Test acc 0.893848\n",
      "Epoch 43. Loss: 0.220949, Train acc 0.920700, Test acc 0.893652\n",
      "Epoch 44. Loss: 0.218790, Train acc 0.921277, Test acc 0.892676\n",
      "Epoch 45. Loss: 0.217121, Train acc 0.921969, Test acc 0.890527\n",
      "Epoch 46. Loss: 0.214358, Train acc 0.923005, Test acc 0.894922\n",
      "Epoch 47. Loss: 0.212183, Train acc 0.923870, Test acc 0.883301\n",
      "Epoch 48. Loss: 0.211855, Train acc 0.923077, Test acc 0.893457\n",
      "Epoch 49. Loss: 0.206956, Train acc 0.925848, Test acc 0.895117\n",
      "Epoch 50. Loss: 0.206259, Train acc 0.926075, Test acc 0.894336\n",
      "Epoch 51. Loss: 0.205050, Train acc 0.925488, Test acc 0.890625\n",
      "Epoch 52. Loss: 0.202838, Train acc 0.927798, Test acc 0.891602\n",
      "Epoch 53. Loss: 0.200488, Train acc 0.928042, Test acc 0.894238\n",
      "Epoch 54. Loss: 0.200661, Train acc 0.928092, Test acc 0.893262\n",
      "Epoch 55. Loss: 0.197770, Train acc 0.929782, Test acc 0.891016\n",
      "Epoch 56. Loss: 0.197796, Train acc 0.928657, Test acc 0.894922\n",
      "Epoch 57. Loss: 0.194232, Train acc 0.929449, Test acc 0.889746\n",
      "Epoch 58. Loss: 0.195355, Train acc 0.930391, Test acc 0.890820\n",
      "Epoch 59. Loss: 0.190497, Train acc 0.932436, Test acc 0.893066\n",
      "Epoch 60. Loss: 0.190667, Train acc 0.931749, Test acc 0.893945\n",
      "Epoch 61. Loss: 0.187999, Train acc 0.932940, Test acc 0.894238\n",
      "Epoch 62. Loss: 0.185336, Train acc 0.933843, Test acc 0.895508\n",
      "Epoch 63. Loss: 0.185433, Train acc 0.933788, Test acc 0.895605\n",
      "Epoch 64. Loss: 0.183572, Train acc 0.935023, Test acc 0.893750\n",
      "Epoch 65. Loss: 0.180069, Train acc 0.935001, Test acc 0.895117\n",
      "Epoch 66. Loss: 0.181482, Train acc 0.935611, Test acc 0.895215\n",
      "Epoch 67. Loss: 0.180671, Train acc 0.936137, Test acc 0.891211\n",
      "Epoch 68. Loss: 0.178132, Train acc 0.937977, Test acc 0.893945\n",
      "Epoch 69. Loss: 0.176118, Train acc 0.937367, Test acc 0.891504\n",
      "Epoch 70. Loss: 0.178315, Train acc 0.936508, Test acc 0.893652\n",
      "Epoch 71. Loss: 0.174464, Train acc 0.938342, Test acc 0.893750\n",
      "Epoch 72. Loss: 0.172216, Train acc 0.939705, Test acc 0.898730\n",
      "Epoch 73. Loss: 0.170998, Train acc 0.940315, Test acc 0.887305\n",
      "Epoch 74. Loss: 0.167546, Train acc 0.942127, Test acc 0.895410\n",
      "Epoch 75. Loss: 0.167816, Train acc 0.941340, Test acc 0.893457\n",
      "Epoch 76. Loss: 0.166302, Train acc 0.941728, Test acc 0.895508\n",
      "Epoch 77. Loss: 0.164581, Train acc 0.943035, Test acc 0.897070\n",
      "Epoch 78. Loss: 0.162817, Train acc 0.943251, Test acc 0.896094\n",
      "Epoch 79. Loss: 0.165020, Train acc 0.942199, Test acc 0.893164\n",
      "Epoch 80. Loss: 0.159237, Train acc 0.944686, Test acc 0.894336\n",
      "Epoch 81. Loss: 0.160326, Train acc 0.944709, Test acc 0.892871\n",
      "Epoch 82. Loss: 0.160031, Train acc 0.944066, Test acc 0.894238\n",
      "Epoch 83. Loss: 0.159450, Train acc 0.944071, Test acc 0.896582\n",
      "Epoch 84. Loss: 0.156152, Train acc 0.945867, Test acc 0.892578\n",
      "Epoch 85. Loss: 0.153023, Train acc 0.947074, Test acc 0.892871\n",
      "Epoch 86. Loss: 0.155466, Train acc 0.945905, Test acc 0.895312\n",
      "Epoch 87. Loss: 0.152388, Train acc 0.947845, Test acc 0.896094\n",
      "Epoch 88. Loss: 0.150831, Train acc 0.948022, Test acc 0.892285\n",
      "Epoch 89. Loss: 0.149186, Train acc 0.948997, Test acc 0.895703\n",
      "Epoch 90. Loss: 0.149141, Train acc 0.948925, Test acc 0.895410\n",
      "Epoch 91. Loss: 0.147555, Train acc 0.949612, Test acc 0.895703\n",
      "Epoch 92. Loss: 0.147784, Train acc 0.949191, Test acc 0.895605\n",
      "Epoch 93. Loss: 0.145385, Train acc 0.949867, Test acc 0.895605\n",
      "Epoch 94. Loss: 0.147696, Train acc 0.948731, Test acc 0.895996\n",
      "Epoch 95. Loss: 0.141420, Train acc 0.951684, Test acc 0.893555\n",
      "Epoch 96. Loss: 0.145806, Train acc 0.949712, Test acc 0.894727\n",
      "Epoch 97. Loss: 0.142715, Train acc 0.951369, Test acc 0.894727\n",
      "Epoch 98. Loss: 0.139038, Train acc 0.952294, Test acc 0.894727\n",
      "Epoch 99. Loss: 0.140479, Train acc 0.951623, Test acc 0.894922\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "for epoch in range(100):\n",
    "    train_loss = 0.\n",
    "    train_acc = 0.\n",
    "    for data, label in train_data:\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        SGD(params, learning_rate/batch_size)\n",
    "\n",
    "        train_loss += nd.mean(loss).asscalar()\n",
    "        train_acc += accuracy(output, label)\n",
    "\n",
    "    test_acc = evaluate_accuracy(test_data, net)\n",
    "    print(\"Epoch %d. Loss: %f, Train acc %f, Test acc %f\" % (epoch, train_loss/len(train_data), train_acc/len(train_data), test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
